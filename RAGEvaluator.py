import streamlit as st
import asyncio
import pandas as pd
from typing import Dict, Any
import time

# LlamaIndex imports
try:
    from llama_index.core.evaluation import (
        FaithfulnessEvaluator,
        RelevancyEvaluator,
        CorrectnessEvaluator,
        SemanticSimilarityEvaluator,
        BatchEvalRunner
    )
    from llama_index.llms.openai import OpenAI
    from llama_index.embeddings.openai import OpenAIEmbedding
    LLAMAINDEX_AVAILABLE = True
except ImportError:
    LLAMAINDEX_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="LLM Evaluation Metrics Dashboard",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

def main():
    st.title("üîç LLM Evaluation Metrics Dashboard")
    st.markdown("Evaluate your LLM responses using multiple metrics with LlamaIndex")
    
    if not LLAMAINDEX_AVAILABLE:
        st.error("""
        **LlamaIndex not installed!**
        
        Please install the required packages:
        ```bash
        pip install llama-index
        pip install llama-index-llms-openai
        pip install llama-index-embeddings-openai
        ```
        """)
        return
    
    # Sidebar for configuration
    st.sidebar.header("‚öôÔ∏è Configuration")
    
    # API Key input
    api_key = st.sidebar.text_input(
        "OpenAI API Key",
        type="password",
        help="Enter your OpenAI API key for evaluation"
    )
    
    # Model selection
    model_choice = st.sidebar.selectbox(
        "Evaluation Model",
        ["gpt-4o","gpt-4", "gpt-4-turbo"],
        help="Choose the model for LLM-as-a-judge evaluation"
    )
    
    # Metrics selection
    st.sidebar.subheader("üìà Metrics to Evaluate")
    eval_faithfulness = st.sidebar.checkbox("Faithfulness", value=True)
    eval_relevancy = st.sidebar.checkbox("Answer Relevancy", value=True)
    eval_correctness = st.sidebar.checkbox("Correctness", value=True)
    eval_semantic_sim = st.sidebar.checkbox("Semantic Similarity", value=True)
    
    # Main input section
    st.header("üìù Input Data")
    
    # Create two columns for better layout
    col1, col2 = st.columns(2)
    
    with col1:
        query = st.text_area(
            "Query/Question",
            height=100,
            placeholder="Enter the original question or query...",
            help="The original question that was asked"
        )
        
        context = st.text_area(
            "Context/Retrieved Documents",
            height=150,
            placeholder="Enter the context or retrieved documents...",
            help="The context or documents that were retrieved for answering the query"
        )
    
    with col2:
        generated_answer = st.text_area(
            "Generated Answer",
            height=100,
            placeholder="Enter the AI-generated answer...",
            help="The answer generated by your LLM system"
        )
        
        reference_answer = st.text_area(
            "Reference Answer (Ground Truth)",
            height=150,
            placeholder="Enter the reference/expected answer...",
            help="The expected or reference answer for comparison"
        )
    
    # Evaluation button
    if st.button("üöÄ Evaluate Metrics", type="primary"):
        if not api_key:
            st.error("Please enter your OpenAI API key in the sidebar")
            return
        
        if not all([query, context, generated_answer, reference_answer]):
            st.error("Please fill in all required fields")
            return
        
        # Run evaluation
        with st.spinner("Computing evaluation metrics..."):
            results = run_evaluation(
                api_key=api_key,
                model=model_choice,
                query=query,
                context=context,
                generated_answer=generated_answer,
                reference_answer=reference_answer,
                eval_faithfulness=eval_faithfulness,
                eval_relevancy=eval_relevancy,
                eval_correctness=eval_correctness,
                eval_semantic_sim=eval_semantic_sim
            )
        
        if results:
            display_results(results)

def run_evaluation(api_key: str, model: str, query: str, context: str, 
                  generated_answer: str, reference_answer: str,
                  eval_faithfulness: bool, eval_relevancy: bool,
                  eval_correctness: bool, eval_semantic_sim: bool) -> Dict[str, Any]:
    """Run the evaluation metrics"""
    
    try:
        # Initialize LLM
        llm = OpenAI(model=model, api_key=api_key)
        
        # Initialize evaluators
        evaluators = {}
        
        if eval_faithfulness:
            evaluators["faithfulness"] = FaithfulnessEvaluator(llm=llm)
        
        if eval_relevancy:
            evaluators["relevancy"] = RelevancyEvaluator(llm=llm)
        
        if eval_correctness:
            evaluators["correctness"] = CorrectnessEvaluator(llm=llm)
        
        if eval_semantic_sim:
            embed_model = OpenAIEmbedding(api_key=api_key)
            evaluators["semantic_similarity"] = SemanticSimilarityEvaluator(
                embed_model=embed_model
            )
        
        # Run evaluations
        results = {}
        
        for metric_name, evaluator in evaluators.items():
            try:
                if metric_name == "faithfulness":
                    result = evaluator.evaluate(
                        query=query,
                        response=generated_answer,
                        contexts=[context]
                    )
                elif metric_name == "relevancy":
                    result = evaluator.evaluate(
                        query=query,
                        response=generated_answer,
                        contexts = [context]
                    )
                elif metric_name == "correctness":
                    result = evaluator.evaluate(
                        query=query,
                        response=generated_answer,
                        reference=reference_answer
                    )
                elif metric_name == "semantic_similarity":
                    result = evaluator.evaluate(
                        response=generated_answer,
                        reference=reference_answer
                    )
                
                results[metric_name] = {
                    "score": result.score,
                    "feedback": getattr(result, 'feedback', 'No feedback available'),
                    "passing": getattr(result, 'passing', None)
                }
                
            except Exception as e:
                results[metric_name] = {
                    "score": None,
                    "feedback": f"Error: {str(e)}",
                    "passing": None
                }
        
        return results
        
    except Exception as e:
        st.error(f"Error during evaluation: {str(e)}")
        return None

def display_results(results: Dict[str, Any]):
    """Display the evaluation results"""
    
    st.header("üìä Evaluation Results")
    
    # Create metrics cards
    cols = st.columns(len(results))
    
    for idx, (metric_name, result) in enumerate(results.items()):
        with cols[idx]:
            score = result["score"]
            passing = result["passing"]
            
            # Format metric name
            metric_display = metric_name.replace("_", " ").title()
            
            # Color coding based on score
            if score is not None:
                # Handle different score ranges
                if score <= 1.0:  # Normalized scores (0-1)
                    display_score = f"{score:.3f}"
                    normalized_score = max(min(score, 1.0), 0.0)
                    if score >= 0.8:
                        color = "green"
                    elif score >= 0.6:
                        color = "orange"
                    else:
                        color = "red"
                else:  # Scores > 1 (like 1-5 scale)
                    display_score = f"{score:.3f}"
                    normalized_score = min(max(score / 5.0, 0.0), 1.0)  # Assume max scale of 5
                    if score >= 4.0:
                        color = "green"
                    elif score >= 3.0:
                        color = "orange"
                    else:
                        color = "red"
                
                st.metric(
                    label=metric_display,
                    value=display_score,
                    delta=f"{'‚úÖ Pass' if passing else '‚ùå Fail'}" if passing is not None else None
                )
                
                # Progress bar (normalize score to 0-1 range)
                st.progress(normalized_score)
                
            else:
                st.metric(
                    label=metric_display,
                    value="Error"
                )
    
    # Detailed feedback section
    st.subheader("üí¨ Detailed Feedback")
    
    for metric_name, result in results.items():
        metric_display = metric_name.replace("_", " ").title()
        
        with st.expander(f"{metric_display} - Detailed Feedback"):
            if result["score"] is not None:
                st.write(f"**Score:** {result['score']:.3f}")
                if result["passing"] is not None:
                    st.write(f"**Status:** {'‚úÖ Pass' if result['passing'] else '‚ùå Fail'}")
                
                st.write("**Feedback:**")
                st.write(result["feedback"])
            else:
                st.error(result["feedback"])
    
    # Summary table
    st.subheader("üìã Summary Table")
    
    summary_data = []
    for metric_name, result in results.items():
        summary_data.append({
            "Metric": metric_name.replace("_", " ").title(),
            "Score": f"{result['score']:.3f}" if result['score'] is not None else "Error",
            "Status": "‚úÖ Pass" if result['passing'] else "‚ùå Fail" if result['passing'] is not None else "N/A",
            "Has Feedback": "Yes" if result['feedback'] and not result['feedback'].startswith("Error") else "No"
        })
    
    df = pd.DataFrame(summary_data)
    st.dataframe(df, use_container_width=True)
    
    # Download results
    csv = df.to_csv(index=False)
    st.download_button(
        label="üì• Download Results as CSV",
        data=csv,
        file_name=f"llm_evaluation_results_{int(time.time())}.csv",
        mime="text/csv"
    )

# Sidebar information
with st.sidebar:
    st.markdown("---")
    st.subheader("‚ÑπÔ∏è About Metrics")
    
    with st.expander("Faithfulness"):
        st.write("""
        Measures whether the generated answer is grounded in the provided context.
        A high score means the answer doesn't contain hallucinations.
        """)
    
    with st.expander("Answer Relevancy"):
        st.write("""
        Evaluates how well the answer addresses the original question.
        Focuses on whether the response is on-topic and helpful.
        """)
    
    with st.expander("Correctness"):
        st.write("""
        Measures the semantic accuracy of the response compared to the reference answer.
        Uses LLM-as-a-judge to evaluate correctness.
        """)
    
    with st.expander("Semantic Similarity"):
        st.write("""
        Computes cosine similarity between generated and reference answers using embeddings.
        Measures semantic closeness of the responses.
        """)

if __name__ == "__main__":
    main()